#' Load mapping data frame
#' 
#' @export
#' 
#' @param path TEMP
#' 
#' @return the core mapping data frame
#' 
#' @importFrom jsonlite fromJSON
#'

load_map <- function(path){  # TODO: integrate to package data
  map <- read.csv(path)
  
  parse_json <- function(x){
    if (x != "") {
      fromJSON(x)
    }
  }
  map$fun_values <- lapply(map$fun_values, parse_json)
  map$fun_values_args <- lapply(map$fun_values_args, parse_json)
  
  return(map)
}


#' Convert a mapping code list stored in a data frame as a string into a lookup table
#' 
#' The string format is analogous to the code_mapping field in standard ARDN SC2 json files around which the standard
#' mapping files for the package will be built. See: https://agmip.github.io/ARDN/Annotation_SC2.html
#' 
#' @export
#' 
#' @param vec a code mapping list [format: list('source1: target1', 'source2: target2')] stored as a string
#' 
#' @return a lookup dataframe with 2 columns: source (original data) and target (standard data)
#' 
#' @importFrom rlang eval_tidy parse_expr
#'

make_code_lookup <- function(vec){
  
  # Ensure vec contains exactly one expression
  if (vec == "1" | length(parse_exprs(vec)) != 1) {
    lkp <- data.frame(source = NA_character_, target = NA_character_)
  } else {
    
    ls <- eval_tidy(parse_expr(vec))
    ls <- strsplit(as.character(ls), ": ")
    ls <- lapply(ls, function(x) list(source = x[1], target = x[2]))
    
    ls_lkp <- lapply(ls, function(lst) {
      if (grepl("c\\(", lst$source)) {
        df <- data.frame(source = strsplit(gsub("[c()]", "", lst$source), ", ")[[1]],
                         target = lst$target)
        return(df)
      } else {
        df <- as.data.frame(do.call(cbind, lst))
        return(df)
      }
      return(ls)
    })
    
    lkp <- as.data.frame(do.call(rbind, ls_lkp))
    
    lkp$target <- ifelse(lkp$target == 1, lkp$source, lkp$target)
  }

  return(lkp)
}

#' Map column headers to standard codes
#' 
#' ###
#' 
#' @export
#' 
#' @param df a data frame of the data to be mapped
#' @param map a lookup table of the categories to be madde to standard codes, as generated by 'made_code_lookup'
#' @param direction a chr stating the direction of the mapping: forward (i.e., to default icasa) or reverse (i.e., from default icasa)
#' 
#' @return a data frame with headers mapped to the annotation format specified in the wrapper function map_data
#'

map_headers <- function(df, map, direction = c("to_icasa", "from_icasa")) {
  
  # df <- template_icasa$INITIAL_CONDITIONS
  # map <- load_map(map_path)
  # map <- map[map$dataModel == output_model & map$icasa_header_short %in% colnames(df),]
  # direction <- "from_icasa"
  
  
  mapped_cols <- c()  # empty vector to store mapped column names = column in both data models, whether same-named of different
  
  if (direction == "to_icasa"){
    map <- map %>% rename(header_in = header, header_out = icasa_header_short)
  } else if (direction == "from_icasa"){
    map <- map %>% rename(header_in = icasa_header_short, header_out = header)
  }
    
  for (i in seq_along(colnames(df))) {

    matches <- which(map$header_in == colnames(df)[i])  # get index of matches
    
    if (length(matches) > 0) {
      for (j in seq_along(matches)) {

        match_index <- matches[j]
        if (is.na(map$header_in[match_index]) | map$header_out[match_index] == "") {
          next
        }
        # Only map headers if they are not identical in input and output models (i.e., as in some cases in ICASA-DSSAT)
        if (map$header_in[match_index] != map$header_out[match_index]){
          mapped_cols <- c(mapped_cols, colnames(df)[i])
          new_col_nm <-map$header_out[match_index]
          df[[new_col_nm]] <- df[[colnames(df)[i]]]  # add mapped column as a new column
        } else {
          mapped_cols <- c(mapped_cols, colnames(df)[i])
          new_col_nm <- paste0(map$header_out[match_index], "_2")
          df[[new_col_nm]] <- df[[colnames(df)[i]]]  # duplicate columns to keep original order
        }
      }
    }
  }

  df <- df[ , !(colnames(df) %in% mapped_cols)]  # remove the original columns after mapping
  colnames(df) <- gsub("_2", "", colnames(df))  # remove temporary suffixes used to preserve column order
  
  out <- list(data = df, mapped = mapped_cols)
  return(out)
} 


#' Map categorical data to standard codes
#' 
#' Currently handles lookup tables as maps rather than json SC2 files, which will be implemented in the future.
#' 
#' @param df a data frame of the data to be mapped
#' @param map a lookup table of the categories to be madde to standard codes, as generated by 'made_code_lookup'
#' 
#' @return a data frame with codes mapped to the format specified in the supplied map
#' 
#' @importFrom dplyr recode_factor
#' @importFrom rlang "!!!"
#'

# TODO: revise: mapping direction + json strings
map_codes <- function(df, map, direction, ...){

  if (direction == "to_icasa"){ # !!run only after mapping headers!
    map <- map %>% rename(header_in = icasa_header_short, header_out = header, unit_in = unit, unit_out = icasa_unit)  
  } else if (direction == "from_icasa"){
    map <- map %>% rename(header_in = header, header_out = icasa_header_short, unit_in = icasa_unit, unit_out = unit)
  }
  
  if (nrow(map) == 0) { # end if map is empty
    return(df)
  }
  
  for (i in 1:nrow(map)){
    
    if (is.na(map$unit_in[i])) {
      next
    }

    if (map$unit_in[i] == "code") {

      header <- map$header_in[i]
      
      mappings <- map$code_mappings[i]
      lookup <- make_code_lookup(mappings)  # TODO: revise using json strings instead of R list()
      var <- df[[header]]
      
      if (all(is.na(lookup))) {
        next
      } else {
        df[[header]] <- recode_factor(var, !!!setNames(as.list(lookup$target), lookup$source), .default = NA_character_)
      }
    }
  }
  return(df)
}


#' Convert numeric vector based on units provided as strings
#' 
#' ####str unit formats
#'  
#' @param x a numeric to be converted
#' @param u1 the input unit, in which x is expressed
#' @param u2 the target unit, in which the output is expressed
#' 
#' @return a converted numeric
#' 
#' @importFrom magrittr "%>%"
#' @importFrom units set_units
#'

convert_unit <- function(x, u1, u2){
  set_units(x, u1, mode = "standard") %>% set_units(u2, mode = "standard") %>% as.numeric()
}

#' Convert units in a data frame based on values stored in map or as attributes
#' 
#' ####s
#' 
#' 
#' @param df ###
#' @param map ###
#' 
#' @return a data frame wherein all numeric columns have beem provided in the target units
#' 
#'

convert_units <- function(df, metadata = NULL, map, direction) {
  
  if (direction == "to_icasa"){ # !!run only after mapping headers!
    map <- map %>% rename(header_in = icasa_header_short, header_out = header, unit_in = unit, unit_out = icasa_unit)  
  } else if (direction == "from_icasa"){
    map <- map %>% rename(header_in = header, header_out = icasa_header_short, unit_in = icasa_unit, unit_out = unit)
  }
  
  # Convert units by checking variable sequentially
  # TODO: check error when colname is not in input data model (may happen in 2-step mapping, e.g., bonares->icasa->dssat)
  for (i in seq_along(colnames(df))) {
    
    for (j in 1:nrow(map)) {
      
      if (map$unit_in[j] == "not_set") {
        if (!is.null(metadata)) {
          map$unit_in[j] <- metadata$unitOfMeasurement_symbol  # retrieve unit in metadata if "not_set" (for sensors)
        } else {
          map$unit_in[j] <- ""
        }
      }
      
      if (colnames(df)[i] == map$header_in[j]){
        if (is.na(map$header_in[j]) | map$unit_out[j] %in% c("","date","code","text","yyyy")) {
          next
        }
        df[[i]] <- convert_unit(df[[i]], map$unit_in[j], map$unit_out[j])
      }
    }
  }
  return(df)
}


#' Apply a wrangling function in the format function(x,df,...) to a dataframe 
#' 
#' ####
#' 
#' @export
#' 
#' @param var a character containing the target variable of the function (x)
#' @param fun a character string containing the function call
#' @param args a list, store additional arguments for the function
#' @param df a dataframe
#' 
#' @return a data frame
#' 
#' 

apply_function <- function(var, fun, args, df) {
  
  fun_parsed <- eval(parse(text = fun))  # parse function
  args_parsed <- if (all(sapply(args, is.null)) | length(args) == 0) { # format argument as simple list
    list()
  } else {
    unlist(args, recursive = FALSE)
  }

  cols <- colnames(df[colnames(df) %in% c(var, args_parsed)])  # save column names for output

  if(is.null(fun_parsed)){
    fun_parsed <- eval(parse(text = "function(x, ...){as.data.frame(identity(x))}"))
  } 
  out <- do.call(fun_parsed, c(list(x = var), list(df = df), args_parsed))
  colnames(out) <- cols          
  
  return(out)
}


#' Apply a set of wrangling functions stored in a map to a data frame
#' 
#' ####
#' 
#' @export
#' 
#' @param df a data frame
#' @param map a data frame, standard map containing the mappings (full map available with load_map())
#' 
#' @return a data frame
#' 
#' 

# TODO: fun_headers, complete missing funs in the map
transform_data <- function(df, map, ...) {
  
  if(all(is.na(map$fun_values) | sapply(map$fun_values, is.null) | map$fun_values == "")){
    return(df)
  }
  
  # Identify argument and transform argument variables
  arg_vars <- map[map$icasa_header_short %in% unlist(map$fun_values_args),]$icasa_header_short
  df_grp <- apply_function(arg_vars, map[map$icasa_header_short%in%arg_vars,]$fun_values, map[map$icasa_header_short%in%arg_vars,]$fun_values_args, df)
  colnames(df_grp) <- colnames(df)
  
  # Transform variables based on updated argument variables
  map <- map[!map$icasa_header_short %in% arg_vars,]
  out_ls <- lapply(1:nrow(map), function(i) {
    apply_function(map$icasa_header_short[i], map$fun_values[i], map$fun_values_args[i], df_grp)
  })
  
  # Merge output into a dataframe
  if(length(out_ls)>1){
    cmn_cols <- Reduce(intersect, lapply(out_ls, colnames))
    out <- unique(
      Reduce(function(x, y) merge(x, y, by = cmn_cols, all = TRUE), out_ls)  # aggregated data
    )
  } else {
    out <- do.call(cbind, out_ls)  # non-aggregative transformed data
  }
  
  return(out)
}


#' Map a data table into a standard format
#' 
#' Currently only handles exact matches (header names, units, and codes). Future versions will handle more complex
#' data mapping (e.g., conditional, concatenation, etc.)
#' 
#' @export
#' 
#' @param df a data frame of the data to be mapped
#' @param tbl_name the name of the focal crop experiment data section in the map
#' @param map a lookup table detailing input and target headers, units, and codes
#' @param keep_unmapped a logical indicating whether to keep unmapped variables in the output
#' @param col_exempt a vector of column names to be kept into the output if keep_unmapped = FALSE
#' 
#' @return a data frame with headers, units, and codes mapped to the format specified in the map
#' 
#' @importFrom dplyr distinct
#'

map_data <- function(df, input_model, output_model, map, keep_unmapped = TRUE, col_exempt = NULL){
  
  # TODO: workaround to skip dataframes with no single match in the output model [currently fails]

  metadata <- attributes(df)$metadata  # store metadata

  # Perform the mapping sequence (i.e., headers -> codes -> units -> transformations)
  mapping_sequence <- function(df, map, ...){
    # Perform the mapping
    if ("icasa" %in% output_model){
      df0 <- df  # store original data
      map <- map[map$dataModel == input_model & map$header %in% colnames(df),]
      headers <- map_headers(df, map, "to_icasa")  # map headers
      df <- headers$data
      mapped_cols <- unique(headers$mapped)
      df <- map_codes(df, map, "to_icasa")  # map codes
      df <- convert_units(df, metadata, map, "to_icasa")  # convert units
      df <- transform_data(df, map)  # apply transformations
    } else if ("icasa" %in% input_model){
      df0 <- df  # store original data
      map <- map[map$dataModel == output_model & map$icasa_header_short %in% colnames(df),]
      headers <- map_headers(df, map, "from_icasa")  # map headers
      df <- headers$data
      mapped_cols <- unique(headers$mapped)  # TODO: CHECK WHY NULL?
      df <- map_codes(df, map, "from_icasa")  # map codes ## TODO: CHECK, PRODUCES ERROR!
      df <- convert_units(df, metadata, map, "from_icasa")  # convert units
    } else {
      df0 <- df  # store original data
      map_icasa <- map[map$dataModel == input_model & map$header %in% colnames(df),]
      map <- map[map$dataModel == output_model & map$icasa_header_short %in% colnames(df),]
      headers <- map_headers(df, , "to_icasa")
      df_icasa <- headers$data
      mapped_cols <- unique(headers$mapped)
      df_icasa <- map_codes(df_icasa, map_icasa, "to_icasa")  # map codes
      df_icasa <- convert_units(df_icasa, map_icasa, "to_icasa")  # convert units  #! METADATA ARG? TOTEST
      df_icasa <- transform_data(df_icasa, map_icasa)  # apply transformations
      ### TODO: test
      df <- map_headers(df_icasa, map, "from_icasa")
      df <- headers$data
      df <- map_codes(df, map, "from_icasa")  # map codes
      df <- convert_units(df, metadata, map, "from_icasa")  # convert units
    }
    # Store unmapped variables
    unmapped_cols <- setdiff(colnames(df0), mapped_cols)  # check main branch
    # Drop columns not in standard if required
    if (keep_unmapped) {
      out <- distinct(df[, !colnames(df) %in% col_exempt])
    } else {
      out <- distinct(df[, !colnames(df) %in% setdiff(unmapped_cols, col_exempt), drop = FALSE])
    }
    return(out)
  }
  
  data_mapped <- mapping_sequence(df, map)  # map data table
  if(!is.null(metadata)){
    attr(data_mapped, "metadata") <- mapping_sequence(metadata, map)  # map metadata table
  }
  
  return(data_mapped)
}



#' Format metadata for different data models
#' 
#' ###
#' 
#' 

fmt_metadata <- function(data, model = c("icasa", "dssat"), section = c("general", "experiment", "management", "soil", "weather")){
  
  if (model == "dssat"){
    if (section == "weather"){
      
      metadata <- attr(data, "metadata")
      if (is.null(metadata)){
        message("Metadata table not found.")
        return(data)
      }
      
      coords <- data.frame(x = mean(metadata$LONG), y = mean(metadata$LAT))  # retrieve coordinates
      addr <- reverse_geocode(coords, long = x, lat = y, method = 'osm', full_results = TRUE, quiet = TRUE)  # find location name
      year <- year(data$DATE[1])  # retrieve year
      wst_id <- toupper(paste0(addr$country_code, abbreviate(addr$town, minlength = 2L)))  # make weather station id (DSSAT format)
      suppressMessages({elev <- get_elev_point(coords, prj = 4326, src = "aws")})
      
      attr(data, "station_metadata") <-
        data.frame(INSI = wst_id,
                   LAT = coords$y,
                   LONG = coords$x,
                   ELEV = elev$elevation,
                   TAV = data %>%
                     summarise(TAV = mean((TMAX + TMIN)/2, na.rm = TRUE)) %>%
                     pull(TAV),
                   AMP = data %>%
                     mutate(mo = month(DATE)) %>%
                     group_by(mo) %>%
                     summarise(mo_TAV = mean((TMAX + TMIN)/2, na.rm = TRUE)) %>%
                     summarise(AMP = (max(mo_TAV)-min(mo_TAV))/2) %>%
                     pull(AMP),
                   REFHT = 2,  # document in device/sensor metadata? Would need text mining workflow
                   WNDHT = NA_real_  # document in device/sensor metadata?
        )
      
      # Top elements
      attr(data, "location") <- toupper(paste(addr$town, addr$state, addr$country, sep = ", "))  # title
      attr(data, "comments") <- list("Sensor metadata: ###", paste0("Dataset generated with csmTools on ", Sys.Date())) ### add sensor/ppty metadata
      attr(data, "filename") <- paste0(wst_id, substr(year, 3, 4), "01", ".WTH")  # file name
      attributes(data)$metadata <- NULL  # remove old format
    }
    ## TODO: ADD ICASA METADATA / OTHER DATA SECTIONS
    if (section == "management"){
      
      metadata <- data$GENERAL
      
      coords <- data.frame(x = mean(data$FIELDS$XCRD), y = mean(data$FIELDS$YCRD))  # retrieve coordinates
      addr <- reverse_geocode(coords, long = x, lat = y, method = 'osm', full_results = TRUE, quiet = TRUE)  # find location name
      year <- year(data$HARVEST[nrow(data$HARVEST),]$HDATE)  # retrieve year
      crop <- data$CULTIVARS$CR
      suppressMessages({elev <- get_elev_point(coords, prj = 4326, src = "aws")})  # retrieve elevation
      
      metadata <- metadata %>%
        mutate(ADDRESS = paste(INSTITUTION, addr$town, addr$state, addr$country, sep = ", "),  # TODO: add more elements in template!
               PEOPLE = paste0(LAST_NAME, ", ", FIRST_NAME, " ", MID_INITIAL),  # TODO: handle missing initials (no NA)
               SITE = paste0(data$FIELDS$FLNAME, ", ", addr$town, ", ", addr$state, ", ", addr$country, "; ", 
                             coords$x, "; ", coords$y, "; ", elev$elevation))
      metadata <- concatenate_per_group(metadata)
      
      data$GENERAL <- metadata
      
      # TODO: autofill experimental design if not done + fix mapping EXP.DETAILS
      filename <- toupper(
        paste0(
          strict_abbreviate("Technische Universität München"),  # TODO: fetch leading/unique institute in metadata
          strict_abbreviate(addr$town),  # site
          substr(year, 3, 4),  # year
          "01",  # TODO: figure out how to find experiment ID
          ".", "WH", "X"  # TODO: file extension + fix code mapping for CROPS
        )
      )
      field_id <- toupper(  # TODO: handle multiple fields
        paste0(
          strict_abbreviate("Technische Universität München"),
          strict_abbreviate(addr$town),
          sprintf("%04d", data$FIELDS$L)
          )
      )
      attr(data, "experiment") <- toupper(paste0(filename, ", ", metadata$EXP.DETAILS, ", ", addr$town, ", ", addr$country, ", ", "3C*4F" ))
      attr(data, "file_name") <- filename
      attr(data, "field_id") <- field_id
      attr(data, "comments") <- NULL ##
    }
    if (section == "soil"){
      
      metadata <- attr(data, "metadata")
      if (is.null(metadata)){
        message("Metadata table not found.")
        return(data)
      }
      
      coords <- data.frame(x = mean(metadata$LONG), y = mean(metadata$LAT))  # retrieve coordinates
      addr <- reverse_geocode(coords, long = x, lat = y, method = 'osm', full_results = TRUE, quiet = TRUE)  # find location name
      institute <- "Technische Universität München"  # TODO: add to metadata
      year <- 2023  # TODO: add to metadata
      soil_nr <- "0001"  # TODO: figure out based on whether institute SOL file exists (if no==>0001, if yes, n+1)
      soil_id <- toupper(
        paste0(
          strict_abbreviate(institute),  # institute
          strict_abbreviate(addr$town),  # site
          substr(year, 3, 4),  # year
          soil_nr)
      )
      
      metadata <- metadata %>%
        mutate(PEDON = soil_id,  # TODO: add more elements in template!
               SOURCE = "SoilGrids", # TODO: attr(data, "soil_source") kept in mapping
               TEXTURE = NA,  # TODO: function to estimate texture based on profile data
               DEPTH = max(data$SLB),
               DESCRIPTION = NA,  # TODO: in fetch metadata
               SITE = addr$town,
               COUNTRY = addr$country,
               SMHB = "IB001", SMPX = "IB001", SMKE = "IB001")
      
      data <- list(SOIL_METADATA = metadata, SOIL_PROFILE_LAYERS = data)
      
      # TODO: autofill experimental design if not done + fix mapping EXP.DETAILS
      filename <- paste0(strict_abbreviate(institute), ".SOL")
      attr(data, "title") <- toupper(paste0(institute, ", ", addr$town, ", ", addr$state, ", ", addr$country))
      attr(data, "file_name") <- filename
      attr(data, "comments") <- NULL ##
      
    }
    
    
    return(data)
  }
}

# # TODO: add_property_mapping <- function(name, unit, icasa = list(), agg_funs = list())
concatenate_per_group <- function(df) {

  constant_cols <- df %>%
    summarise(across(everything(), ~ n_distinct(.) == 1)) %>% 
    select_if(~ .) %>%
    colnames()
  
  other_cols <- setdiff(colnames(df), constant_cols)
  
  out <- df %>% 
    group_by(across(all_of(constant_cols))) %>%
    summarise(across(all_of(other_cols), ~ paste(., collapse = ", ")), .groups = 'drop')
  
  return(out)
}


strict_abbreviate <- function(string, minlength = 2) {
  abbr <- abbreviate(string, minlength = minlength, strict = TRUE)
  if (nchar(abbr) > minlength) {
    abbr <- substr(abbr, 1, minlength)
  }
  return(abbr)
}
